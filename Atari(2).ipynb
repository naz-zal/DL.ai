{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Atari(2).ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"1FHKo1_5uP6Q","colab_type":"code","colab":{}},"cell_type":"code","source":["# this is the second attempt at RL and the results after training for one hour has been saved"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KZpzHBO_uP6b","colab_type":"code","colab":{}},"cell_type":"code","source":["import gym\n","from gym.wrappers import Monitor\n","import itertools\n","import numpy as np\n","import os\n","import random\n","import sys\n","import psutil\n","import tensorflow as tf\n","\n","from lib import plotting\n","from collections import deque, namedtuple"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QXzTQo7WuP6i","colab_type":"code","colab":{}},"cell_type":"code","source":["env = gym.envs.make(\"Breakout-v0\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F2I9_IMfuP6p","colab_type":"code","colab":{}},"cell_type":"code","source":["#Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n","VALID_ACTIONS = [0, 1, 2, 3]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KYh1VKHDuP6v","colab_type":"code","colab":{}},"cell_type":"code","source":["class StateProcessor():\n","    \n","    def __init__(self):\n","        \n","        with tf.variable_scope(\"state_processor\"):\n","            self.input_state = tf.placeholder(shape = [210, 160, 3], dtype = tf.uint8)\n","            self.output = tf.image.rgb_to_grayscale(self.input_state)\n","            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n","            self.output = tf.image.resize_images(self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","            self.output = tf.squeeze(self.output)\n","            \n","    def process(self, sess, state):\n","        \n","        \"\"\"\n","        Args:\n","            state: A [210, 160, 3] Atari RGB State\n","            \n","        Returns:\n","            A processed [84, 84] state representing grayscale values.\n","        \n","        \"\"\"\n","        \n","        return sess.run(self.output, {self.input_state: state})"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ps4zCmCzuP63","colab_type":"code","colab":{}},"cell_type":"code","source":["class Estimator():\n","    \n","    #Q-Value Estimator neural network.\n","    \n","    def __init__(self, scope=\"estimator\", summaries_dir=None):\n","        self.scope = scope\n","        # Writes Tensorboard summaries to disk\n","        self.summary_writer = None\n","        with tf.variable_scope(scope):\n","            # Build the graph\n","            self._build_model()\n","            if summaries_dir:\n","                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n","                if not os.path.exists(summary_dir):\n","                    os.makedirs(summary_dir)\n","                self.summary_writer = tf.summary.FileWriter(summary_dir)\n","    \n","    def _build_model(self):\n","        \n","        #the input is 4 RGB Atari frames of shape [84, 84]\n","        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype = tf.uint8, name = \"X\")\n","        #the target value (Q value of taking an action from state)\n","        self.y_pl = tf.placeholder(shape = [None], dtype=tf.float32, name=\"y\")\n","        # Integer id of which action was selected\n","        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n","        \n","        X = tf.to_float(self.X_pl) / 255.0\n","        batch_size = tf.shape(self.X_pl)[0]\n","        \n","        # Three convolutional layers\n","        conv1 = tf.contrib.layers.conv2d(\n","            X, 32, 8, 4, activation_fn=tf.nn.relu)\n","        conv2 = tf.contrib.layers.conv2d(\n","            conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n","        conv3 = tf.contrib.layers.conv2d(\n","            conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n","        \n","        #two FC layers \n","        flattened = tf.contrib.layers.flatten(conv3)\n","        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n","        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n","        \n","        #alright i did take some time off to understand why this next step was taken\n","        #but basically, it's like masking the output with just the inputs that were fed in, bc \n","        # you dun need errthang. the keras equivalent of this is \n","        #filtered_output = keras.layers.merge([output, actions_input], mode='mul')\n","        \n","        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n","        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n","    \n","        #calculating the loss now \n","        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n","        self.loss = tf.reduce_mean(self.losses)\n","        \n","        # Optimizer Parameters from original paper\n","        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n","        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n","        \n","        # Summaries for Tensorboard\n","        self.summaries = tf.summary.merge([\n","            tf.summary.scalar(\"loss\", self.loss),\n","            tf.summary.histogram(\"loss_hist\", self.losses),\n","            tf.summary.histogram(\"q_values_hist\", self.predictions),\n","            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n","        ])\n","        \n","        \n","    def predict(self, sess, s):\n","        \"\"\"\n","        Predicts action values.\n","\n","        Args:\n","          sess: Tensorflow session\n","          s: State input of shape [batch_size, 4, 160, 160, 3]\n","\n","        Returns:\n","          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n","          action values.\n","        \"\"\"\n","        return sess.run(self.predictions, { self.X_pl: s })\n","        \n","    # so to my understanding, you do all the shite in _build_model. if you noticed, build_model does not return anything\n","    # but does declare many variables which are then later used, and also used in update function. \n","    def update(self, sess, s, a, y):\n","        \"\"\"\n","        Updates the estimator towards the given targets.\n","\n","        Args:\n","          sess: Tensorflow session object\n","          s: State input of shape [batch_size, 4, 160, 160, 3]\n","          a: Chosen actions of shape [batch_size]\n","          y: Targets of shape [batch_size]\n","\n","        Returns:\n","          The calculated loss on the batch.\n","        \"\"\"\n","        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n","        summaries, global_step, _, loss = sess.run(\n","            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n","            feed_dict)\n","        if self.summary_writer:\n","            self.summary_writer.add_summary(summaries, global_step)\n","        return loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"M3SROXnFuP69","colab_type":"code","colab":{},"outputId":"91089b78-8587-44d7-e365-8985a003bcb1"},"cell_type":"code","source":["tf.reset_default_graph()\n","global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n","\n","e = Estimator(scope=\"test\")\n","sp = StateProcessor()\n","\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    \n","    # Example observation batch\n","    observation = env.reset()\n","    \n","    observation_p = sp.process(sess, observation)\n","    observation = np.stack([observation_p] * 4, axis=2)\n","    observations = np.array([observation] * 2)\n","    \n","    # Test Prediction\n","    print(e.predict(sess, observations))\n","\n","    # Test training step\n","    y = np.array([10.0, 10.0])\n","    a = np.array([1, 3])\n","    print(e.update(sess, observations, a, y))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-5-41366e8d9705>:57: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please switch to tf.train.get_global_step\n"],"name":"stdout"},{"output_type":"stream","text":["/home/onepanel/.conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["[[0.01948194 0.02805924 0.00774765 0.0075382 ]\n"," [0.01948194 0.02805924 0.00774765 0.0075382 ]]\n","99.644455\n"],"name":"stdout"}]},{"metadata":{"id":"6Qp6igGmuP7M","colab_type":"code","colab":{}},"cell_type":"code","source":["class ModelParametersCopier():\n","    \"\"\"\n","    Copy model parameters of one estimator to another.\n","    \"\"\"\n","    \n","    def __init__(self, estimator1, estimator2):\n","        \"\"\"\n","        Defines copy-work operation graph.  \n","        Args:\n","          estimator1: Estimator to copy the paramters from\n","          estimator2: Estimator to copy the parameters to\n","        \"\"\"\n","        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n","        e1_params = sorted(e1_params, key=lambda v: v.name)\n","        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n","        e2_params = sorted(e2_params, key=lambda v: v.name)\n","\n","        self.update_ops = []\n","        for e1_v, e2_v in zip(e1_params, e2_params):\n","            op = e2_v.assign(e1_v)\n","            self.update_ops.append(op)\n","            \n","    def make(self, sess):\n","        \"\"\"\n","        Makes copy.\n","        Args:\n","            sess: Tensorflow session instance\n","        \"\"\"\n","        sess.run(self.update_ops)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"J7W9HlhPuP7U","colab_type":"code","colab":{}},"cell_type":"code","source":["def make_epsilon_greedy_policy(estimator, nA):\n","    \"\"\"\n","    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n","\n","    Args:\n","        estimator: An estimator that returns q values for a given state\n","        nA: Number of actions in the environment.\n","\n","    Returns:\n","        A function that takes the (sess, observation, epsilon) as an argument and returns\n","        the probabilities for each action in the form of a numpy array of length nA.\n","\n","    \"\"\"\n","    def policy_fn(sess, observation, epsilon):\n","        A = np.ones(nA, dtype=float) * epsilon / nA\n","        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n","        best_action = np.argmax(q_values)\n","        A[best_action] += (1.0 - epsilon)\n","        return A\n","    return policy_fn"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_9kCob61uP7c","colab_type":"code","colab":{}},"cell_type":"code","source":["def deep_q_learning(sess,\n","                    env,\n","                    q_estimator,\n","                    target_estimator,\n","                    state_processor,\n","                    num_episodes,\n","                    experiment_dir,\n","                    replay_memory_size=500000,\n","                    replay_memory_init_size=50000,\n","                    update_target_estimator_every=10000,\n","                    discount_factor=0.99,\n","                    epsilon_start=1.0,\n","                    epsilon_end=0.1,\n","                    epsilon_decay_steps=500000,\n","                    batch_size=32,\n","                    record_video_every=50):\n","    \"\"\"\n","    Q-Learning algorithm for off-policy TD control using Function Approximation.\n","    Finds the optimal greedy policy while following an epsilon-greedy policy.\n","\n","    Args:\n","        sess: Tensorflow Session object\n","        env: OpenAI environment\n","        q_estimator: Estimator object used for the q values\n","        target_estimator: Estimator object used for the targets\n","        state_processor: A StateProcessor object\n","        num_episodes: Number of episodes to run for\n","        experiment_dir: Directory to save Tensorflow summaries in\n","        replay_memory_size: Size of the replay memory\n","        replay_memory_init_size: Number of random experiences to sampel when initializing \n","          the reply memory.\n","        update_target_estimator_every: Copy parameters from the Q estimator to the \n","          target estimator every N steps\n","        discount_factor: Gamma discount factor\n","        epsilon_start: Chance to sample a random action when taking an action.\n","          Epsilon is decayed over time and this is the start value\n","        epsilon_end: The final minimum value of epsilon after decaying is done\n","        epsilon_decay_steps: Number of steps to decay epsilon over\n","        batch_size: Size of batches to sample from the replay memory\n","        record_video_every: Record a video every N episodes\n","\n","    Returns:\n","        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n","    \"\"\"\n","\n","    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n","\n","    # The replay memory\n","    replay_memory = []\n","    \n","    # Make model copier object\n","    estimator_copy = ModelParametersCopier(q_estimator, target_estimator)\n","\n","    # Keeps track of useful statistics\n","    stats = plotting.EpisodeStats(\n","        episode_lengths=np.zeros(num_episodes),\n","        episode_rewards=np.zeros(num_episodes))\n","    \n","    # For 'system/' summaries, usefull to check if currrent process looks healthy\n","    current_process = psutil.Process()\n","\n","    # Create directories for checkpoints and summaries\n","    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n","    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n","    monitor_path = os.path.join(experiment_dir, \"monitor\")\n","    \n","    if not os.path.exists(checkpoint_dir):\n","        os.makedirs(checkpoint_dir)\n","    if not os.path.exists(monitor_path):\n","        os.makedirs(monitor_path)\n","\n","    saver = tf.train.Saver()\n","    # Load a previous checkpoint if we find one\n","    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n","    if latest_checkpoint:\n","        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n","        saver.restore(sess, latest_checkpoint)\n","    \n","    # Get the current time step\n","    total_t = sess.run(tf.contrib.framework.get_global_step())\n","\n","    # The epsilon decay schedule\n","    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n","\n","    # The policy we're following\n","    policy = make_epsilon_greedy_policy(\n","        q_estimator,\n","        len(VALID_ACTIONS))\n","\n","    # Populate the replay memory with initial experience\n","    print(\"Populating replay memory...\")\n","    state = env.reset()\n","    state = state_processor.process(sess, state)\n","    state = np.stack([state] * 4, axis=2)\n","    for i in range(replay_memory_init_size):\n","        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n","        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n","        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n","        next_state = state_processor.process(sess, next_state)\n","        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n","        replay_memory.append(Transition(state, action, reward, next_state, done))\n","        if done:\n","            state = env.reset()\n","            state = state_processor.process(sess, state)\n","            state = np.stack([state] * 4, axis=2)\n","        else:\n","            state = next_state\n","\n","\n","    # Record videos\n","    # Add env Monitor wrapper\n","    env = Monitor(env, directory=monitor_path, video_callable=lambda count: count % record_video_every == 0, resume=True)\n","\n","    for i_episode in range(num_episodes):\n","\n","        # Save the current checkpoint\n","        saver.save(tf.get_default_session(), checkpoint_path)\n","\n","        # Reset the environment\n","        state = env.reset()\n","        state = state_processor.process(sess, state)\n","        state = np.stack([state] * 4, axis=2)\n","        loss = None\n","\n","        # One step in the environment\n","        for t in itertools.count():\n","\n","            # Epsilon for this time step\n","            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n","\n","            # Maybe update the target estimator\n","            if total_t % update_target_estimator_every == 0:\n","                estimator_copy.make(sess)\n","                print(\"\\nCopied model parameters to target network.\")\n","\n","            # Print out which step we're on, useful for debugging.\n","            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n","                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n","            sys.stdout.flush()\n","\n","            # Take a step\n","            action_probs = policy(sess, state, epsilon)\n","            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n","            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n","            next_state = state_processor.process(sess, next_state)\n","            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n","\n","            # If our replay memory is full, pop the first element\n","            if len(replay_memory) == replay_memory_size:\n","                replay_memory.pop(0)\n","\n","            # Save transition to replay memory\n","            replay_memory.append(Transition(state, action, reward, next_state, done))   \n","\n","            # Update statistics\n","            stats.episode_rewards[i_episode] += reward\n","            stats.episode_lengths[i_episode] = t\n","\n","            # Sample a minibatch from the replay memory\n","            samples = random.sample(replay_memory, batch_size)\n","            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n","\n","            # Calculate q values and targets\n","            q_values_next = target_estimator.predict(sess, next_states_batch)\n","            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n","\n","            # Perform gradient descent update\n","            states_batch = np.array(states_batch)\n","            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n","\n","            if done:\n","                break\n","\n","            state = next_state\n","            total_t += 1\n","\n","        # Add summaries to tensorboard\n","        episode_summary = tf.Summary()\n","        episode_summary.value.add(simple_value=epsilon, tag=\"episode/epsilon\")\n","        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], tag=\"episode/reward\")\n","        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], tag=\"episode/length\")\n","        episode_summary.value.add(simple_value=current_process.cpu_percent(), tag=\"system/cpu_usage_percent\")\n","        episode_summary.value.add(simple_value=current_process.memory_percent(memtype=\"vms\"), tag=\"system/v_memeory_usage_percent\")\n","        q_estimator.summary_writer.add_summary(episode_summary, i_episode)\n","        q_estimator.summary_writer.flush()\n","        \n","        yield total_t, plotting.EpisodeStats(\n","            episode_lengths=stats.episode_lengths[:i_episode+1],\n","            episode_rewards=stats.episode_rewards[:i_episode+1])\n","\n","    return stats"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y8Qcoa0PuP7v","colab_type":"code","colab":{},"outputId":"304fa4cb-7aa4-4cd9-b645-1f6a16ac910a"},"cell_type":"code","source":["tf.reset_default_graph()\n","\n","# Where we save our checkpoints and graphs\n","experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n","\n","# Create a glboal step variable\n","global_step = tf.Variable(0, name='global_step', trainable=False)\n","    \n","# Create estimators\n","q_estimator = Estimator(scope=\"q_estimator\", summaries_dir=experiment_dir)\n","target_estimator = Estimator(scope=\"target_q\")\n","\n","# State processor\n","state_processor = StateProcessor()\n","\n","# Run it!\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    for t, stats in deep_q_learning(sess,\n","                                    env,\n","                                    q_estimator=q_estimator,\n","                                    target_estimator=target_estimator,\n","                                    state_processor=state_processor,\n","                                    experiment_dir=experiment_dir,\n","                                    num_episodes=10000,\n","                                    replay_memory_size=500000,\n","                                    replay_memory_init_size=50000,\n","                                    update_target_estimator_every=10000,\n","                                    epsilon_start=1.0,\n","                                    epsilon_end=0.1,\n","                                    epsilon_decay_steps=500000,\n","                                    discount_factor=0.99,\n","                                    batch_size=32):\n","\n","        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/home/onepanel/.conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Populating replay memory...\n","\n","Copied model parameters to target network.\n","Step 166 (166) @ Episode 1/10000, loss: 0.0336284339427948113\n","Episode Reward: 0.0\n","Step 186 (352) @ Episode 2/10000, loss: 0.0026276651769876486\n","Episode Reward: 0.0\n","Step 315 (667) @ Episode 3/10000, loss: 0.0334563851356506358\n","Episode Reward: 3.0\n","Step 156 (823) @ Episode 4/10000, loss: 0.0021483036689460278\n","Episode Reward: 0.0\n","Step 211 (1034) @ Episode 5/10000, loss: 0.0028206310234963894\n","Episode Reward: 1.0\n","Step 254 (1288) @ Episode 6/10000, loss: 0.0011526406742632397\n","Episode Reward: 1.0\n","Step 208 (1496) @ Episode 7/10000, loss: 0.00136469490826129915\n","Episode Reward: 1.0\n","Step 180 (1676) @ Episode 8/10000, loss: 0.0008548716432414949\n","Episode Reward: 0.0\n","Step 242 (1918) @ Episode 9/10000, loss: 0.00048314375453628687\n","Episode Reward: 1.0\n","Step 275 (2193) @ Episode 10/10000, loss: 0.0012946641072630882\n","Episode Reward: 2.0\n","Step 276 (2469) @ Episode 11/10000, loss: 0.00127367116510868077\n","Episode Reward: 2.0\n","Step 163 (2632) @ Episode 12/10000, loss: 0.0012547041988000274\n","Episode Reward: 0.0\n","Step 465 (3097) @ Episode 13/10000, loss: 0.06847729533910751066\n","Episode Reward: 5.0\n","Step 166 (3263) @ Episode 14/10000, loss: 0.0007826022338122129\n","Episode Reward: 0.0\n","Step 272 (3535) @ Episode 15/10000, loss: 0.0013286010362207893\n","Episode Reward: 2.0\n","Step 332 (3867) @ Episode 16/10000, loss: 0.00168744253460317855\n","Episode Reward: 3.0\n","Step 425 (4292) @ Episode 17/10000, loss: 0.0320183187723159857\n","Episode Reward: 5.0\n","Step 267 (4559) @ Episode 18/10000, loss: 0.03647372499108314517\n","Episode Reward: 2.0\n","Step 301 (4860) @ Episode 19/10000, loss: 0.03269962593913078054\n","Episode Reward: 3.0\n","Step 170 (5030) @ Episode 20/10000, loss: 0.0016327989287674427\n","Episode Reward: 0.0\n","Step 215 (5245) @ Episode 21/10000, loss: 0.00105778663419187076\n","Episode Reward: 1.0\n","Step 173 (5418) @ Episode 22/10000, loss: 0.0328466258943080942\n","Episode Reward: 0.0\n","Step 191 (5609) @ Episode 23/10000, loss: 0.0011858373181894422\n","Episode Reward: 0.0\n","Step 170 (5779) @ Episode 24/10000, loss: 0.0016693323850631714\n","Episode Reward: 0.0\n","Step 236 (6015) @ Episode 25/10000, loss: 0.00206247507594525855\n","Episode Reward: 1.0\n","Step 256 (6271) @ Episode 26/10000, loss: 0.00129925925284624173\n","Episode Reward: 1.0\n","Step 215 (6486) @ Episode 27/10000, loss: 0.0635385587811472682\n","Episode Reward: 1.0\n","Step 200 (6686) @ Episode 28/10000, loss: 0.0014216124545782804\n","Episode Reward: 1.0\n","Step 387 (7073) @ Episode 29/10000, loss: 0.0014714417047798634\n","Episode Reward: 4.0\n","Step 233 (7306) @ Episode 30/10000, loss: 0.5294952392578125493\n","Episode Reward: 1.0\n","Step 319 (7625) @ Episode 31/10000, loss: 0.0018250755965709686\n","Episode Reward: 3.0\n","Step 284 (7909) @ Episode 32/10000, loss: 0.0008663118351250887\n","Episode Reward: 2.0\n","Step 171 (8080) @ Episode 33/10000, loss: 0.0009877111297100782\n","Episode Reward: 0.0\n","Step 173 (8253) @ Episode 34/10000, loss: 0.00139455380849540234\n","Episode Reward: 0.0\n","Step 209 (8462) @ Episode 35/10000, loss: 0.0017622888553887606\n","Episode Reward: 1.0\n","Step 332 (8794) @ Episode 36/10000, loss: 0.0011755204759538174\n","Episode Reward: 3.0\n","Step 252 (9046) @ Episode 37/10000, loss: 0.03639357537031174866\n","Episode Reward: 2.0\n","Step 230 (9276) @ Episode 38/10000, loss: 0.00162944546900689624\n","Episode Reward: 1.0\n","Step 179 (9455) @ Episode 39/10000, loss: 0.0009560110047459602\n","Episode Reward: 0.0\n","Step 391 (9846) @ Episode 40/10000, loss: 0.00140519149135798227\n","Episode Reward: 4.0\n","Step 153 (9999) @ Episode 41/10000, loss: 0.0018545018974691638\n","Copied model parameters to target network.\n","Step 227 (10073) @ Episode 41/10000, loss: 0.03255125135183334422\n","Episode Reward: 1.0\n","Step 164 (10237) @ Episode 42/10000, loss: 0.0010295968968421222\n","Episode Reward: 0.0\n","Step 213 (10450) @ Episode 43/10000, loss: 0.00036945860483683646\n","Episode Reward: 1.0\n","Step 169 (10619) @ Episode 44/10000, loss: 0.00020549356122501194\n","Episode Reward: 0.0\n","Step 309 (10928) @ Episode 45/10000, loss: 0.0326547883450984953\n","Episode Reward: 2.0\n","Step 175 (11103) @ Episode 46/10000, loss: 0.0013697938993573189\n","Episode Reward: 0.0\n","Step 186 (11289) @ Episode 47/10000, loss: 0.00125599419698119163\n","Episode Reward: 0.0\n","Step 301 (11590) @ Episode 48/10000, loss: 0.0013759343419224024\n","Episode Reward: 2.0\n","Step 206 (11796) @ Episode 49/10000, loss: 0.00139204761944711285\n","Episode Reward: 1.0\n","Step 340 (12136) @ Episode 50/10000, loss: 0.00133103900589048866\n","Episode Reward: 3.0\n","Step 303 (12439) @ Episode 51/10000, loss: 0.00193832465447485453\n","Episode Reward: 2.0\n","Step 303 (12742) @ Episode 52/10000, loss: 0.03767940029501915763\n","Episode Reward: 2.0\n","Step 167 (12909) @ Episode 53/10000, loss: 0.03196996450424194247\n","Episode Reward: 0.0\n","Step 175 (13084) @ Episode 54/10000, loss: 0.0010562215466052294\n","Episode Reward: 0.0\n","Step 182 (13266) @ Episode 55/10000, loss: 0.00105967163108289246\n","Episode Reward: 0.0\n","Step 172 (13438) @ Episode 56/10000, loss: 0.0322201885282993373\n","Episode Reward: 0.0\n","Step 269 (13707) @ Episode 57/10000, loss: 0.00066443800460547215\n","Episode Reward: 2.0\n","Step 238 (13945) @ Episode 58/10000, loss: 0.0020118812099099165\n","Episode Reward: 1.0\n","Step 402 (14347) @ Episode 59/10000, loss: 0.03222343325614929487\n","Episode Reward: 4.0\n","Step 382 (14729) @ Episode 60/10000, loss: 0.00068445864599198155\n","Episode Reward: 4.0\n","Step 294 (15023) @ Episode 61/10000, loss: 0.00205872999504208565\n","Episode Reward: 2.0\n","Step 225 (15248) @ Episode 62/10000, loss: 0.0008537106914445758\n","Episode Reward: 1.0\n","Step 253 (15501) @ Episode 63/10000, loss: 0.0320719778537750244\n","Episode Reward: 2.0\n","Step 233 (15734) @ Episode 64/10000, loss: 0.00203460967168211944\n","Episode Reward: 1.0\n","Step 281 (16015) @ Episode 65/10000, loss: 0.0016750944778323174\n","Episode Reward: 2.0\n","Step 233 (16248) @ Episode 66/10000, loss: 0.0375988483428955137\n","Episode Reward: 1.0\n","Step 238 (16486) @ Episode 67/10000, loss: 0.0013782141031697392\n","Episode Reward: 1.0\n","Step 316 (16802) @ Episode 68/10000, loss: 0.0021909738425165415\n","Episode Reward: 3.0\n","Step 205 (17007) @ Episode 69/10000, loss: 0.00035398779436945915\n","Episode Reward: 1.0\n","Step 162 (17169) @ Episode 70/10000, loss: 0.0012278457870706916\n","Episode Reward: 0.0\n","Step 256 (17425) @ Episode 71/10000, loss: 0.03728787600994111746\n","Episode Reward: 1.0\n","Step 248 (17673) @ Episode 72/10000, loss: 0.0010456568561494351\n","Episode Reward: 1.0\n","Step 242 (17915) @ Episode 73/10000, loss: 0.0017266381764784455\n","Episode Reward: 1.0\n","Step 314 (18229) @ Episode 74/10000, loss: 0.03118155337870121447\n","Episode Reward: 2.0\n","Step 271 (18500) @ Episode 75/10000, loss: 0.0020060725510120399\n","Episode Reward: 2.0\n","Step 344 (18844) @ Episode 76/10000, loss: 0.0019383276812732228\n","Episode Reward: 3.0\n","Step 176 (19020) @ Episode 77/10000, loss: 0.0016828358639031649\n","Episode Reward: 0.0\n","Step 303 (19323) @ Episode 78/10000, loss: 0.00169295375235378745\n","Episode Reward: 2.0\n","Step 200 (19523) @ Episode 79/10000, loss: 0.0016769587527960539\n","Episode Reward: 1.0\n","Step 177 (19700) @ Episode 80/10000, loss: 0.0018614659784361724\n","Episode Reward: 0.0\n","Step 222 (19922) @ Episode 81/10000, loss: 0.0015513930702582002\n","Episode Reward: 1.0\n","Step 77 (19999) @ Episode 82/10000, loss: 0.0005404148832894862\n","Copied model parameters to target network.\n","Step 200 (20122) @ Episode 82/10000, loss: 0.0016207306180149317\n","Episode Reward: 1.0\n","Step 190 (20312) @ Episode 83/10000, loss: 0.03910203650593757623\n","Episode Reward: 0.0\n","Step 205 (20517) @ Episode 84/10000, loss: 0.0020432677119970321\n","Episode Reward: 1.0\n","Step 281 (20798) @ Episode 85/10000, loss: 0.00174577860161662157\n","Episode Reward: 2.0\n","Step 265 (21063) @ Episode 86/10000, loss: 0.0020429519936442375\n","Episode Reward: 2.0\n","Step 409 (21472) @ Episode 87/10000, loss: 0.00106713303830474624\n","Episode Reward: 4.0\n","Step 289 (21761) @ Episode 88/10000, loss: 0.00213984679430723286\n","Episode Reward: 2.0\n","Step 254 (22015) @ Episode 89/10000, loss: 0.0021204999648034573\n","Episode Reward: 1.0\n","Step 238 (22253) @ Episode 90/10000, loss: 0.00119008414912968874\n","Episode Reward: 1.0\n","Step 358 (22611) @ Episode 91/10000, loss: 0.0010149964364245534\n","Episode Reward: 3.0\n","Step 308 (22919) @ Episode 92/10000, loss: 0.0008670673123560846\n","Episode Reward: 2.0\n","Step 437 (23356) @ Episode 93/10000, loss: 0.00123064836952835325\n","Episode Reward: 4.0\n","Step 345 (23701) @ Episode 94/10000, loss: 0.0021048036869615316\n","Episode Reward: 3.0\n","Step 241 (23942) @ Episode 95/10000, loss: 0.0015519520966336131\n","Episode Reward: 1.0\n","Step 234 (24176) @ Episode 96/10000, loss: 0.0015423720469698313\n","Episode Reward: 1.0\n","Step 232 (24408) @ Episode 97/10000, loss: 0.0016624893760308623\n","Episode Reward: 1.0\n","Step 184 (24592) @ Episode 98/10000, loss: 0.0012596447486430407\n","Episode Reward: 0.0\n","Step 409 (25001) @ Episode 99/10000, loss: 0.0010094175813719632\n","Episode Reward: 4.0\n","Step 170 (25171) @ Episode 100/10000, loss: 0.00167408748529851444\n","Episode Reward: 0.0\n","Step 250 (25421) @ Episode 101/10000, loss: 0.0008724491926841438\n","Episode Reward: 1.0\n","Step 202 (25623) @ Episode 102/10000, loss: 0.0324271433055400859\n","Episode Reward: 1.0\n","Step 272 (25895) @ Episode 103/10000, loss: 0.0021408405154943466\n","Episode Reward: 2.0\n","Step 181 (26076) @ Episode 104/10000, loss: 0.0012553333071991801\n","Episode Reward: 0.0\n","Step 171 (26247) @ Episode 105/10000, loss: 0.0010475888848304749\n","Episode Reward: 0.0\n","Step 279 (26526) @ Episode 106/10000, loss: 0.00184504385106265546\n","Episode Reward: 2.0\n","Step 240 (26766) @ Episode 107/10000, loss: 0.00229073269292712255\n","Episode Reward: 1.0\n","Step 182 (26948) @ Episode 108/10000, loss: 0.0010496551403775811\n","Episode Reward: 0.0\n","Step 305 (27253) @ Episode 109/10000, loss: 0.00183852040208876136\n","Episode Reward: 2.0\n","Step 303 (27556) @ Episode 110/10000, loss: 0.0381897203624248512\n","Episode Reward: 2.0\n","Step 176 (27732) @ Episode 111/10000, loss: 0.0010307431221008309\n","Episode Reward: 0.0\n","Step 170 (27902) @ Episode 112/10000, loss: 0.0016520968638360589\n","Episode Reward: 0.0\n","Step 298 (28200) @ Episode 113/10000, loss: 0.0016787730855867267\n","Episode Reward: 2.0\n","Step 323 (28523) @ Episode 114/10000, loss: 0.0320761464536199036\n","Episode Reward: 3.0\n","Step 246 (28769) @ Episode 115/10000, loss: 0.0014256816357374191\n","Episode Reward: 2.0\n","Step 341 (29110) @ Episode 116/10000, loss: 0.00190182449296116836\n","Episode Reward: 3.0\n","Step 280 (29390) @ Episode 117/10000, loss: 0.00145678734406828885\n","Episode Reward: 2.0\n","Step 178 (29568) @ Episode 118/10000, loss: 0.0023496602661907673\n","Episode Reward: 0.0\n","Step 220 (29788) @ Episode 119/10000, loss: 0.00192857161164283755\n","Episode Reward: 1.0\n","Step 211 (29999) @ Episode 120/10000, loss: 0.0315636917948722845\n","Copied model parameters to target network.\n","Step 232 (30020) @ Episode 120/10000, loss: 0.0031913970597088337\n","Episode Reward: 1.0\n","Step 245 (30265) @ Episode 121/10000, loss: 0.0023576996754854918\n","Episode Reward: 1.0\n","Step 284 (30549) @ Episode 122/10000, loss: 0.0012475551338866353\n","Episode Reward: 2.0\n","Step 206 (30755) @ Episode 123/10000, loss: 0.0020075177308171988\n","Episode Reward: 1.0\n","Step 270 (31025) @ Episode 124/10000, loss: 0.0415257103741169182\n","Episode Reward: 2.0\n","Step 172 (31197) @ Episode 125/10000, loss: 0.0021142400801181793\n","Episode Reward: 0.0\n","Step 237 (31434) @ Episode 126/10000, loss: 0.0013056180905550718\n","Episode Reward: 1.0\n","Step 298 (31732) @ Episode 127/10000, loss: 0.0018479521386325366\n","Episode Reward: 2.0\n","Step 600 (32332) @ Episode 128/10000, loss: 0.0034854705445468426\n","Episode Reward: 8.0\n","Step 335 (32667) @ Episode 129/10000, loss: 0.0020668625365942717\n","Episode Reward: 3.0\n","Step 156 (32823) @ Episode 130/10000, loss: 0.0313784629106521627\n","Episode Reward: 0.0\n","Step 312 (33135) @ Episode 131/10000, loss: 0.0032164510339498528\n","Episode Reward: 2.0\n","Step 334 (33469) @ Episode 132/10000, loss: 0.0298721548169851393\n","Episode Reward: 3.0\n","Step 275 (33744) @ Episode 133/10000, loss: 0.0250517651438713075\n","Episode Reward: 2.0\n","Step 179 (33923) @ Episode 134/10000, loss: 0.0013707180041819813\n","Episode Reward: 0.0\n","Step 104 (34027) @ Episode 135/10000, loss: 0.0022323671728372574"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-be11090fb7f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                                     \u001b[0mepsilon_decay_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                     \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                     batch_size=32):\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEpisode Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-dd1f41199636>\u001b[0m in \u001b[0;36mdeep_q_learning\u001b[0;34m(sess, env, q_estimator, target_estimator, state_processor, num_episodes, experiment_dir, replay_memory_size, replay_memory_init_size, update_target_estimator_every, discount_factor, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size, record_video_every)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;31m# Perform gradient descent update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mstates_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-41366e8d9705>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, sess, s, a, y)\u001b[0m\n\u001b[1;32m     98\u001b[0m         summaries, global_step, _, loss = sess.run(\n\u001b[1;32m     99\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             feed_dict)\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"ohMW_59SuP79","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"5Uz32E3KuP8D","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"1gj42lATuP8N","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"Z8JhvSrkuP8U","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}